[
{
	"uri": "https://necrose01.github.io/Workshop/vi/1.-introduction/",
	"title": "1. Giới thiệu",
	"tags": [],
	"description": "",
	"content": "NLP là gì? Xử lý ngôn ngữ tự nhiên (Natural Language Processing – NLP) là lĩnh vực của trí tuệ nhân tạo tập trung vào việc giúp máy tính hiểu, diễn giải và tạo ra ngôn ngữ của con người.\nNLP được ứng dụng rộng rãi trong nhiều sản phẩm và dịch vụ, ví dụ:\nChatbot và trợ lý ảo Dịch máy (Machine Translation) Tìm kiếm thông minh Phân tích cảm xúc (Sentiment Analysis) Tóm tắt văn bản tự động Phát hiện chủ đề và phân loại nội dung AWS Comprehend AWS Comprehend là dịch vụ xử lý ngôn ngữ tự nhiên hoàn toàn được quản lý (fully managed service) từ AWS, cho phép bạn trích xuất thông tin và insight từ văn bản.\nMột số khả năng chính:\nPhân tích cảm xúc (Sentiment Analysis) Nhận dạng thực thể (Entity Recognition) Phát hiện ngôn ngữ Phân loại văn bản (Custom Classification) Trích xuất từ khóa và cụm từ Với AWS Comprehend, bạn có thể triển khai các tác vụ NLP mà không cần tự huấn luyện mô hình từ đầu, đồng thời vẫn hỗ trợ tùy chỉnh để phù hợp dữ liệu của bạn.\nTransformers Transformers là kiến trúc mô hình học sâu (Deep Learning Architecture) dựa trên cơ chế Self-Attention, được giới thiệu lần đầu trong bài báo “Attention Is All You Need” (2017).\nTransformers hiện là nền tảng của nhiều mô hình ngôn ngữ tiên tiến như BERT, GPT, T5, RoBERTa.\nMột số ưu điểm:\nXử lý song song tốt hơn RNN/LSTM Hiệu quả với chuỗi dữ liệu dài Khả năng học ngữ cảnh và quan hệ từ trong câu Các loại Transformers phổ biến:\nEncoder-only (BERT, RoBERTa) Decoder-only (GPT, LLaMA) Encoder-Decoder (T5, BART) AWS Transformers trên SageMaker AWS cung cấp khả năng huấn luyện và triển khai các mô hình Transformers thông qua Amazon SageMaker.\nBạn có thể:\nSử dụng thư viện Hugging Face Transformers tích hợp sẵn trên SageMaker Tùy chỉnh và fine-tune mô hình theo dữ liệu riêng Triển khai endpoint phục vụ dự đoán thời gian thực (real-time inference) hoặc theo batch Việc kết hợp SageMaker và AWS Comprehend cho phép xây dựng pipeline NLP linh hoạt, vừa tận dụng sức mạnh mô hình tùy chỉnh, vừa khai thác các API NLP có sẵn từ AWS.\n"
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/4.-transformers/4.1-create-ec2/",
	"title": "Cài đặt EC2",
	"tags": [],
	"description": "",
	"content": "1. Chuẩn bị EC2 Instance Truy cập AWS Management Console → chọn EC2 → Launch Instance.\nĐặt tên cho Instance: aws-transformers-model.\nChọn AMI: Ubuntu Server 22.04 LTS (Free tier eligible).\nChọn Instance type: t2.micro (Free tier).\nĐể huấn luyện mô hình Transformers với dữ liệu lớn hoặc yêu cầu tính toán cao, nên chọn các loại instance có GPU hoặc CPU mạnh hơn như:\ng4dn.xlarge, g5.xlarge (GPU instance)\np3.2xlarge, p4d.24xlarge (Instance chuyên cho deep learning)\nc5.4xlarge, c6i.4xlarge (CPU tối ưu hiệu năng cao)\nỞ đây, chúng ta dùng t2.micro để chạy demo training với dữ liệu nhỏ nhằm tiết kiệm chi phí và phù hợp free tier.\nKey pair: Chọn hoặc tạo mới key pair (.pem file), nhớ tải và lưu lại để SSH vào instance.\nNetwork: Cấu hình Security Group cho phép.\nChọn toàn bộ 3 mục:\nAllow SSH traffic from Allow HTTPS traffic from the internet Allow HTTP traffic from the internet Storage: Để mặc định là 8 GB.\nNhấn Launch instance để khởi động.\n"
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/3.-aws-comprehend/3.1-demo-aws-comprehend/",
	"title": "Demo sơ bộ AWS Comprehend",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Thực hiện một demo nhanh với AWS Comprehend thông qua AWS Console để phân tích cảm xúc và trích xuất thông tin từ đoạn văn bản mẫu.\nCác bước thực hiện 1. Mở AWS Comprehend Đăng nhập vào AWS Management Console. Tìm và chọn dịch vụ Amazon Comprehend.\nChạy dịch vụ Amazon Comprehend. Ở đây có thể chạy thử đoạn văn bản mẫu có sẵn để xem cách AWS Comprehend phân tích văn bản, bấm Analyze để chạy phân tích. Sau khi phân tích, có thể tùy chọn xem thông tin phân tích văn bản: Entities: Phân tích thực thể Key phrases: Phân tích các từ khóa Language: Nhận diện ngôn ngữ PII: Nhận diện thông tin cá nhân Sentiment: Phân tích cảm xúc Targeted sentiment: Phân tích cảm xúc của đối tượng cụ thể Syntax: Phân tích cú pháp "
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/",
	"title": "Natural Language Processing với Comprehend và Transformers",
	"tags": [],
	"description": "",
	"content": "Natural Language Processing với Comprehend và Transformers Tổng quan Bài lab này hướng dẫn bạn tìm hiểu và thực hành Xử lý ngôn ngữ tự nhiên (Natural Language Processing – NLP) bằng cách kết hợp AWS Comprehend và mô hình Transformers.\nThông qua nội dung workshop, bạn sẽ áp dụng các kỹ thuật này để phân tích đánh giá khách hàng trên một sàn thương mại điện tử, từ đó rút ra thông tin hữu ích phục vụ cải thiện sản phẩm và dịch vụ.\nNội dung chính Giới thiệu Chuẩn bị AWS Comprehend Transformers Dọn dẹp tài nguyên "
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/2.-prepare/2.1-create-s3-bucket/",
	"title": "Tạo Amazon S3 bucket",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tạo một Amazon S3 bucket để lưu trữ dữ liệu đầu vào (tệp CSV) và kết quả phân tích của workshop.\nCác bước thực hiện Đăng nhập vào AWS Management Console.\nTruy cập dịch vụ Amazon S3 từ thanh tìm kiếm.\nChọn Create bucket.\nNhập tên bucket (ví dụ: aws-nlp-data).\nGiữ nguyên các thiết lập mặc định khác hoặc tùy chỉnh theo nhu cầu.\nChọn Create bucket để hoàn tất.\nKết quả mong đợi Một bucket mới xuất hiện trong danh sách Amazon S3 buckets. Bucket sẵn sàng để tải lên dữ liệu CSV. "
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/2.-prepare/",
	"title": "2. Chuẩn bị",
	"tags": [],
	"description": "",
	"content": "Chuẩn bị Trước khi bắt đầu workshop, hãy đảm bảo bạn đã sẵn sàng với các tài nguyên và dữ liệu sau:\nAmazon S3 bucket để lưu trữ dữ liệu và kết quả phân tích. Tệp CSV chứa dữ liệu đánh giá khách hàng (bao gồm các trường: tên khách hàng, số sao, sản phẩm, loại sản phẩm, ngày đánh giá, nội dung đánh giá, số lượt thích). Cấu hình IAM policy Tạo và gán quyền cần thiết để AWS Comprehend và mô hình Transformers có thể truy cập S3, đọc dữ liệu và ghi kết quả xử lý. "
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/2.-prepare/2.2-create-csv-file/",
	"title": "Chuẩn bị và tải tệp CSV lên S3",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Chuẩn bị tệp CSV chứa dữ liệu đánh giá khách hàng từ nguồn cơ sở dữ liệu hoặc file sẵn có, đảm bảo đúng định dạng để AWS Comprehend và mô hình Transformers có thể xử lý. Sau đó tải tệp CSV này lên bucket S3 đã tạo ở bước 1.1.\nPhần 1 – Cấu hình tệp CSV Cấu trúc các cột dữ liệu\nTệp CSV cần có các cột sau:\ncustomer_name – Tên khách hàng rating – Số sao đánh giá (1–5) product_name – Tên sản phẩm product_category – Loại sản phẩm review_date – Ngày đánh giá (YYYY-MM-DD) review_text – Nội dung đánh giá likes – Số lượt thích bình luận Để tối ưu mô hình, có thể tối giản csv lại, chỉ chứa những dữ liệu văn bản cần thiết cho mô hình, tránh 1 số văn bản không cần thiết, ở đây file csv chỉ giữ lại 2 cột khách hàng và đánh giá.\nMẫu CSV \u0026#34;customer\u0026#34;,\u0026#34;review\u0026#34; \u0026#34;Nguyen Van A\u0026#34;,\u0026#34;The joystick is very responsive, gaming feels amazing, worth the money!\u0026#34; \u0026#34;Le Thi B\u0026#34;,\u0026#34;Beautiful, sturdy, works well with both PC and phone.\u0026#34; \u0026#34;Tran Van C\u0026#34;,\u0026#34;Broke after just a few days of use, extremely disappointed!\u0026#34; \u0026#34;Pham Thi D\u0026#34;,\u0026#34;Weak battery, unstable Bluetooth connection.\u0026#34; \u0026#34;Vo Quoc E\u0026#34;,\u0026#34;.\u0026#34; \u0026#34;Ngo My F\u0026#34;,\u0026#34;??\u0026#34; \u0026#34;Do Nhat G\u0026#34;,\u0026#34;I just had pho, it was delicious!\u0026#34; \u0026#34;Hoang T H\u0026#34;,\u0026#34;The weather is so nice today that I decided to buy it.\u0026#34; \u0026#34;Trinh Manh I\u0026#34;,\u0026#34;D*mn this joystick makes me so mad!\u0026#34; \u0026#34;Bui Gia K\u0026#34;,\u0026#34;Trash! Selling like this will get you in serious trouble!!!\u0026#34; Định dạng \u0026amp; Encoding\nĐịnh dạng: .csv (comma-separated values) Encoding: UTF-8 để tránh lỗi font tiếng Việt Dòng đầu tiên: chứa tiêu đề cột (header row) Phần 2 – Chuyển đổi bảng DB sang CSV Nếu dữ liệu nằm trong cơ sở dữ liệu (MySQL, PostgreSQL, v.v.), bạn có thể xuất sang CSV:\nMySQL SELECT customer_name, review_text FROM customer_reviews INTO OUTFILE \u0026#39;/tmp/customer_reviews.csv\u0026#39; FIELDS TERMINATED BY \u0026#39;,\u0026#39; ENCLOSED BY \u0026#39;\u0026#34;\u0026#39; LINES TERMINATED BY \u0026#39;\\n\u0026#39;; "
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/4.-transformers/4.2-setup-environment/",
	"title": "Setup Environment trên EC2",
	"tags": [],
	"description": "",
	"content": "Kết nối tới EC2 Instance và cài đặt môi trường Truy cập AWS Management Console → chọn EC2 → Chọn Instance vừa tạo → Connect.\nCập nhật package và cài đặt pip, git, curl. Sau khi chạy chờ 1 lúc để quá trình cập nhật và cài đặt hoàn tất.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y sudo apt install -y python3-pip git curl 3. Cài đặt Transformers \u0026amp; dependencies\nCài đặt môi trường python3 -m venv venv source venv/bin/activate Cài đặt Transformers pip install --upgrade pip pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu pip install transformers datasets scikit-learn pandas 4. Tải File CSV từ S3 vào\ncài đặt AWS CLI # tải AWS CLI curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; # tải trình giải nén sudo apt install unzip # giải nén file vừa tải unzip awscliv2.zip # cài đặt AWS CLI sudo ./aws/install # Kiểm tra xem AWS CLI đã cài chưa aws --version Tải file CSV về Instance: Bạn cần đảm bảo user có quyền truy cập vào S3 tối thiểu (AmazonS3ReadOnlyAccess) đảm bảo Instance cần gán IAM role cho phép truy cập vào S3, nếu Instance chưa có sẽ không thể truy cập vào S3 để tải file\n# cấu hình aws: Ở đây chỉ nhập phần region là khu vực đang sử dụng (vd:ap-southeast-1), còn lại để trống. aws configure # tải file CSV về aws s3 cp s3://aws-nlp-data/processed/summary_reviews_20250809T124112Z.csv "
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/3.-aws-comprehend/3.2-create-lambda-function/",
	"title": "Tạo hàm lambda xử lý file CSV",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tự động chạy khi có file CSV mới được upload vào S3. Đọc từng dòng trong CSV, gọi Comprehend DetectSentiment để phân tích cảm xúc của review. Tạo file CSV kết quả và lưu vào thư mục processed/ trong cùng bucket hoặc bucket khác. Bước 1: Tạo hàm Lambda Truy cập AWS Lambda Console → Create function.\nChọn Author from scratch:\nName: ComprehendCSVProcessor Runtime: Python 3.13 (hoặc bản mới nhất) Role: Chọn NLP-Lambda-Role đã tạo ở phần 2.4. Nhấn Create function.\nBước 2: Code Lambda Dán đoạn code sau vào Code source của Lambda: import boto3 import csv import io import json import os import urllib.parse from datetime import datetime # CONFIG - change according to your environment REGION = \u0026#34;ap-southeast-1\u0026#34; LANGUAGE_CODE = \u0026#34;en\u0026#34; SENSITIVE_WORDS = { # sample list; can be loaded from S3 if you need a larger list \u0026#34;damn\u0026#34;, \u0026#34;shit\u0026#34;, \u0026#34;trash\u0026#34;, \u0026#34;bastard\u0026#34;, \u0026#34;f**k\u0026#34;, \u0026#34;m*\u0026#34; } # Initialize clients s3 = boto3.client(\u0026#34;s3\u0026#34;, region_name=REGION) comprehend = boto3.client(\u0026#34;comprehend\u0026#34;, region_name=REGION) # Map sentiment string -\u0026gt; numeric label (used for training) SENTIMENT_TO_LABEL = { \u0026#34;POSITIVE\u0026#34;: 0, \u0026#34;NEGATIVE\u0026#34;: 1, \u0026#34;NEUTRAL\u0026#34;: 2, \u0026#34;MIXED\u0026#34;: 3, \u0026#34;N/A\u0026#34;: 4 } def normalize_token(t: str): return t.strip().lower() def analyze_text(text: str): \u0026#34;\u0026#34;\u0026#34; Call Comprehend APIs and return a detailed result dictionary. \u0026#34;\u0026#34;\u0026#34; if not text or text.strip() == \u0026#34;\u0026#34;: return { \u0026#34;sentiment\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;sentiment_scores\u0026#34;: {\u0026#34;Positive\u0026#34;: 0, \u0026#34;Negative\u0026#34;: 0, \u0026#34;Neutral\u0026#34;: 0, \u0026#34;Mixed\u0026#34;: 0}, \u0026#34;key_phrases\u0026#34;: [], \u0026#34;entities\u0026#34;: [], \u0026#34;syntax_tokens\u0026#34;: [] } sent = comprehend.detect_sentiment(Text=text, LanguageCode=LANGUAGE_CODE) kp = comprehend.detect_key_phrases(Text=text, LanguageCode=LANGUAGE_CODE) ents = comprehend.detect_entities(Text=text, LanguageCode=LANGUAGE_CODE) syntax = comprehend.detect_syntax(Text=text, LanguageCode=LANGUAGE_CODE) return { \u0026#34;sentiment\u0026#34;: sent.get(\u0026#34;Sentiment\u0026#34;), \u0026#34;sentiment_scores\u0026#34;: sent.get(\u0026#34;SentimentScore\u0026#34;, {}), \u0026#34;key_phrases\u0026#34;: kp.get(\u0026#34;KeyPhrases\u0026#34;, []), \u0026#34;entities\u0026#34;: ents.get(\u0026#34;Entities\u0026#34;, []), \u0026#34;syntax_tokens\u0026#34;: syntax.get(\u0026#34;SyntaxTokens\u0026#34;, []) } def mark_sensitive_tokens(syntax_tokens): \u0026#34;\u0026#34;\u0026#34; From Comprehend syntax tokens, mark tokens found in SENSITIVE_WORDS. Returns a list of token dicts: {\u0026#34;text\u0026#34;:..., \u0026#34;beginOffset\u0026#34;:..., \u0026#34;part_of_speech\u0026#34;:..., \u0026#34;is_sensitive\u0026#34;: True/False} \u0026#34;\u0026#34;\u0026#34; out = [] for tk in syntax_tokens: txt = tk.get(\u0026#34;Text\u0026#34;, \u0026#34;\u0026#34;) pos_tag = tk.get(\u0026#34;PartOfSpeech\u0026#34;, {}).get(\u0026#34;Tag\u0026#34;) is_sens = normalize_token(txt) in SENSITIVE_WORDS out.append({ \u0026#34;text\u0026#34;: txt, \u0026#34;beginOffset\u0026#34;: tk.get(\u0026#34;BeginOffset\u0026#34;), \u0026#34;part_of_speech\u0026#34;: pos_tag, \u0026#34;is_sensitive\u0026#34;: is_sens }) return out def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34; Lambda entry point expects an S3 Put event. For manual testing, craft an event with \u0026#39;bucket\u0026#39; and \u0026#39;key\u0026#39;. \u0026#34;\u0026#34;\u0026#34; try: record = event[\u0026#34;Records\u0026#34;][0] bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = urllib.parse.unquote_plus(record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;]) except Exception as e: return {\u0026#34;statusCode\u0026#34;: 400, \u0026#34;body\u0026#34;: f\u0026#34;Invalid event format: {e}\u0026#34;} base_name = os.path.basename(key) timestamp = datetime.utcnow().strftime(\u0026#34;%Y%m%dT%H%M%SZ\u0026#34;) # Temporary local paths download_path = f\u0026#34;/tmp/{base_name}\u0026#34; output_jsonl_path = f\u0026#34;/tmp/processed_{base_name.rsplit(\u0026#39;.\u0026#39;,1)[0]}_{timestamp}.jsonl\u0026#34; output_csv_path = f\u0026#34;/tmp/summary_{base_name.rsplit(\u0026#39;.\u0026#39;,1)[0]}_{timestamp}.csv\u0026#34; # Download CSV from S3 try: s3.download_file(bucket, key, download_path) except Exception as e: return {\u0026#34;statusCode\u0026#34;: 500, \u0026#34;body\u0026#34;: f\u0026#34;Error downloading s3://{bucket}/{key} - {e}\u0026#34;} # Open CSV and process each row jsonl_file = open(output_jsonl_path, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) csv_buffer = io.StringIO() csv_writer = csv.writer(csv_buffer, quoting=csv.QUOTE_MINIMAL) csv_writer.writerow([\u0026#34;id\u0026#34;, \u0026#34;customer\u0026#34;, \u0026#34;review\u0026#34;, \u0026#34;label\u0026#34;, \u0026#34;label_str\u0026#34;, \u0026#34;top_key_phrases\u0026#34;, \u0026#34;sensitive_words\u0026#34;]) processed_count = 0 with open(download_path, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as csvfile: reader = csv.DictReader(csvfile) for idx, row in enumerate(reader): processed_count += 1 doc_id = f\u0026#34;{base_name}_{idx}\u0026#34; customer = row.get(\u0026#34;customer\u0026#34;, \u0026#34;\u0026#34;) review_text = row.get(\u0026#34;review\u0026#34;, \u0026#34;\u0026#34;) # Call Comprehend result = analyze_text(review_text) # Token-level sensitive marking tokens = mark_sensitive_tokens(result.get(\u0026#34;syntax_tokens\u0026#34;, [])) # Collect sensitive words found sensitive_found = sorted(list({t[\u0026#34;text\u0026#34;] for t in tokens if t[\u0026#34;is_sensitive\u0026#34;]})) # Collect top key phrases key_phrases_list = [kp.get(\u0026#34;Text\u0026#34;) for kp in result.get(\u0026#34;key_phrases\u0026#34;, [])] entities_list = [{\u0026#34;text\u0026#34;: e.get(\u0026#34;Text\u0026#34;), \u0026#34;type\u0026#34;: e.get(\u0026#34;Type\u0026#34;)} for e in result.get(\u0026#34;entities\u0026#34;, [])] # Document-level label label_str = result.get(\u0026#34;sentiment\u0026#34;, \u0026#34;N/A\u0026#34;) label_num = SENTIMENT_TO_LABEL.get(label_str, 4) # JSONL entry doc = { \u0026#34;id\u0026#34;: doc_id, \u0026#34;customer\u0026#34;: customer, \u0026#34;text\u0026#34;: review_text, \u0026#34;label\u0026#34;: label_num, \u0026#34;label_str\u0026#34;: label_str, \u0026#34;sentiment_scores\u0026#34;: result.get(\u0026#34;sentiment_scores\u0026#34;, {}), \u0026#34;key_phrases\u0026#34;: key_phrases_list, \u0026#34;entities\u0026#34;: entities_list, \u0026#34;tokens\u0026#34;: tokens } jsonl_file.write(json.dumps(doc, ensure_ascii=False) + \u0026#34;\\n\u0026#34;) # CSV summary row csv_writer.writerow([ doc_id, customer, review_text, label_num, label_str, \u0026#34;|\u0026#34;.join(key_phrases_list), \u0026#34;|\u0026#34;.join(sensitive_found) ]) jsonl_file.close() # Save CSV file locally as well with open(output_csv_path, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;, newline=\u0026#34;\u0026#34;) as f: f.write(csv_buffer.getvalue()) # Upload outputs back to S3 output_jsonl_key = f\u0026#34;processed/{os.path.basename(output_jsonl_path)}\u0026#34; output_csv_key = f\u0026#34;processed/{os.path.basename(output_csv_path)}\u0026#34; try: s3.upload_file(output_jsonl_path, bucket, output_jsonl_key) s3.upload_file(output_csv_path, bucket, output_csv_key) except Exception as e: return {\u0026#34;statusCode\u0026#34;: 500, \u0026#34;body\u0026#34;: f\u0026#34;Error uploading results to S3: {e}\u0026#34;} return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;Processing complete\u0026#34;, \u0026#34;processed_count\u0026#34;: processed_count, \u0026#34;jsonl_s3\u0026#34;: f\u0026#34;s3://{bucket}/{output_jsonl_key}\u0026#34;, \u0026#34;csv_s3\u0026#34;: f\u0026#34;s3://{bucket}/{output_csv_key}\u0026#34; } } Chọn Deploy (hoặc nhấn Ctrl + Shift + U) để lưu và triển khai code.\nBước 3: Tạo file test event Mở AWS Lambda Console → chọn function ComprehendCSVProcessor. Nhấn Test → chọn Create new event. Chọn Event template: tìm \u0026quot;Amazon S3 Put\u0026quot; (hoặc \u0026quot;S3 Put\u0026quot;). Chỉnh JSON như sau (thay tên bucket và file key của bạn): { \u0026#34;Records\u0026#34;: [ { \u0026#34;s3\u0026#34;: { \u0026#34;bucket\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bucket-name\u0026#34; }, \u0026#34;object\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;reviews.csv\u0026#34; } } } ] } \u0026ldquo;key\u0026rdquo; phải đúng đường dẫn \u0026amp; tên file trong S3 (ví dụ \u0026ldquo;input/customer_reviews.csv\u0026rdquo;). Nếu tên file có dấu cách hoặc ký tự đặc biệt, nó sẽ được encode (space → %20). Hãy copy chính xác từ S3 console.\nBước 4: Chạy test và kiểm tra kết quả Nhấn Test → Lambda sẽ đọc file từ S3 và xử lý. Sau khi chạy xong: Mở S3 Console. Vào thư mục processed/ để xem file CSV kết quả. Ở đây xuất ra 2 file jsonl để training và file csv để linh hoạt sử dụng.\n"
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/4.-transformers/4.3-training-model/",
	"title": "Training model",
	"tags": [],
	"description": "",
	"content": "1. Tạo file train.py Mở trình soạn thảo trên EC2 (ví dụ nano hoặc vim): nano train.py Giao diện văn bản nano: Copy Source code sau, sau đó lưu lại import pandas as pd from sklearn.model_selection import train_test_split from datasets import Dataset from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer import torch # 1. Load dataset CSV df = pd.read_csv(\u0026#34;summary_reviews_20250809T124112Z.csv\u0026#34;) # sửa tên file CSV tải từ S3 df = df.rename(columns={\u0026#34;label\u0026#34;: \u0026#34;labels\u0026#34;}) # Transformers cần tên \u0026#34;labels\u0026#34; # 2. Train/Test split train_df, test_df = train_test_split(df, test_size=0.2, random_state=42) train_dataset = Dataset.from_pandas(train_df) test_dataset = Dataset.from_pandas(test_df) # 3. Load tokenizer \u0026amp; model nhỏ cho t2.micro model_name = \u0026#34;distilbert-base-uncased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3) # 4. Tokenize def tokenize(batch): return tokenizer(batch[\u0026#34;review\u0026#34;], padding=True, truncation=True, max_length=64) train_dataset = train_dataset.map(tokenize, batched=True) test_dataset = test_dataset.map(tokenize, batched=True) # 5. Training arguments training_args = TrainingArguments( output_dir=\u0026#34;./results\u0026#34;, # evaluation_strategy=\u0026#34;epoch\u0026#34;, num_train_epochs=3, per_device_train_batch_size=1, per_device_eval_batch_size=1, logging_dir=\u0026#34;./logs\u0026#34;, logging_steps=1 ) # 6. Metrics from sklearn.metrics import accuracy_score, precision_recall_fscore_support def compute_metrics(p): preds = p.predictions.argmax(-1) precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average=\u0026#34;weighted\u0026#34;) acc = accuracy_score(p.label_ids, preds) return {\u0026#34;accuracy\u0026#34;: acc, \u0026#34;precision\u0026#34;: precision, \u0026#34;recall\u0026#34;: recall, \u0026#34;f1\u0026#34;: f1} # 7. Trainer trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset, tokenizer=tokenizer, compute_metrics=compute_metrics ) # 8. Train \u0026amp; Evaluate trainer.train() metrics = trainer.evaluate() print(metrics) # 9. Save model trainer.save_model(\u0026#34;./final_model\u0026#34;) print(\u0026#34;Model saved to ./final_model\u0026#34;) 2. Chạy train.py Cài lại đầy đủ transformers pip install --upgrade \u0026#34;transformers[torch]\u0026#34; accelerate Chạy lệnh sau trong thư mục chứa train.py và file CSV. Sẽ mất 1 lúc để quá trình training hoàn thành. Vì Instance sử dụng t2.micro rất yếu, nên nhiều trường hợp khi chạy training giữa chừng sẽ bị sập(killed). Trường hợp này cần phải nâng cấp instance type lên cao hơn và mở rộng bộ nhớ hơn. Nếu như quá trình training bị sập, hãy thử nâng cấp instance type từ từ lên như t2.medium, xong tới t2.large để giảm chi phí phát sinh. Trong trường hợp mọi instance type chi phí rẻ không thể training xong được, thì bắt buộc phải nâng cấp lên type giá cao hơn. Về căn bản chạy training model NLP vốn đã đòi hỏi CPU và GPU mạnh nên chuyện phát sinh chi phí là chuyện bắt buộc. Hoặc có thể chạy training model trên máy local nếu như máy local đủ khỏe để chạy.\nVì bài lab chỉ hướng dẫn cách train cho model, nên data dùng để training cũng quá ít để model có thể học, nên việc overfit là rất bình thường. Nếu như muốn traning hiệu quả thì cần đưa vào lượng lớn data trong 1 lần training, thay vì training nhỏ giọt từng data ít ỏi này.\npython3 train.py 3. Kết quả Training Model Quá trình training mô hình phân loại văn bản với distilbert-base-uncased đã được thực hiện thành công trên EC2 instance với dataset nhỏ.\nMô hình đã được train đủ 3 epoch, tổng thời gian training khoảng 7.7 giây trên instance t2.micro.\nLoss trong quá trình training có sự biến động, cho thấy mô hình đã học được thông tin từ dữ liệu (train_loss cuối cùng là khoảng 0.94).\nSau khi training, mô hình được lưu lại thành công trong thư mục ./final_model.\nKết quả đánh giá mô hình trên tập test: Metric Giá trị Loss 0.875 Accuracy 0.5 Precision 0.25 Recall 0.5 F1-score 0.33 Một số trọng số của lớp phân loại (classifier) được khởi tạo mới vì mô hình gốc chưa có phần này, nên việc train thêm trên task cụ thể là cần thiết. Các cảnh báo về metric không xác định do dữ liệu nhỏ và phân bố nhãn không cân bằng, đây là hiện tượng bình thường trong các bài toán với dữ liệu giới hạn. Độ chính xác và các chỉ số còn thấp, nguyên nhân chính do kích thước dataset nhỏ, tuy nhiên mô hình đã đạt được khả năng học cơ bản. Quá trình training và lưu model hoàn toàn thành công, mô hình đã sẵn sàng cho các bước fine-tune hoặc inference tiếp theo. 4. Upload nội dung lên S3 Upload final_model đã train xong lên S3 aws s3 cp ./final_model s3://aws-nlp-data/transformers-demo/ --recursive final_model đã được đưa lên S3, có thể download về hoặc mở ra để xem, và sử dụng, hoặc tiếp tục train. "
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/3.-aws-comprehend/",
	"title": "3. AWS Comprehend",
	"tags": [],
	"description": "",
	"content": "Tổng quan Amazon Comprehend là dịch vụ xử lý ngôn ngữ tự nhiên (NLP – Natural Language Processing) được quản lý hoàn toàn bởi AWS, giúp phân tích và trích xuất thông tin từ văn bản mà không cần xây dựng hoặc huấn luyện mô hình AI phức tạp.\nComprehend sử dụng công nghệ machine learning để tự động nhận diện:\nNgôn ngữ của văn bản. Cảm xúc (Sentiment) – tích cực, tiêu cực, trung tính, hỗn hợp. Thực thể (Entities) – tên người, địa điểm, tổ chức, ngày tháng,\u0026hellip; Từ khóa chính (Key phrases). Chủ đề (Topics) và phân loại văn bản. "
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/2.-prepare/2.3-upload-csv-to-s3-bucket/",
	"title": "Upload CSV lên S3",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tải tệp CSV chứa dữ liệu đánh giá khách hàng lên Amazon S3 bucket đã tạo, và xác nhận dữ liệu đã được lưu trữ thành công.\nCác bước thực hiện Mở Amazon S3 trong AWS Management Console.\nChọn bucket đã tạo ở bước 1.1 – Tạo Amazon S3 bucket.\nNhấn Upload.\nTrong mục Add files, chọn tệp CSV đã chuẩn bị ở bước 1.2 – Chuẩn bị và cấu hình CSV.\nGiữ nguyên các thiết lập mặc định (hoặc tùy chỉnh nếu cần), sau đó nhấn Upload để bắt đầu tải lên.\nXác nhận dữ liệu đã upload Sau khi upload thành công, bạn sẽ thấy tệp CSV xuất hiện trong danh sách Objects của bucket.\nKết quả mong đợi Tệp CSV hiển thị trong danh sách object của bucket S3. Có thể xem/kiểm tra nội dung tệp trực tiếp từ S3. "
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/4.-transformers/",
	"title": "4 . Transformers",
	"tags": [],
	"description": "",
	"content": "Tổng quan Transformers là một kiến trúc mô hình học sâu (deep learning) chuyên biệt cho xử lý ngôn ngữ tự nhiên (NLP) và hiện là nền tảng của nhiều mô hình AI tiên tiến như BERT, GPT, T5, RoBERTa.\nƯu điểm nổi bật của Transformers là khả năng xử lý song song và nắm bắt ngữ cảnh dài, giúp tăng tốc huấn luyện và cải thiện chất lượng dự đoán.\nNhắc lại về Amazon Comprehend Amazon Comprehend đã đủ mạnh để thực hiện các tác vụ phân tích văn bản tổng quát (vì bản chất cũng là một dịch vụ NLP).\nTuy nhiên, để cá nhân hóa và tối ưu cho một mục đích cụ thể như phân tích dữ liệu thương mại điện tử, chúng ta sẽ cần sử dụng Transformers để huấn luyện và tinh chỉnh mô hình, giúp phân tích sâu hơn và phù hợp hơn với đặc thù ngành.\nChúng ta sẽ:\nSử dụng Amazon EC2 để huấn luyện một mô hình Transformers cho phân tích thương mại. Lưu và quản lý mô hình trên S3 để tái sử dụng hoặc triển khai. Có thể sử dụng SageMaker Notebook để huấn luyện mô hình. SageMaker thân thiện hơn, tích hợp nhiều công cụ tối ưu và hiệu quả hơn EC2 cho tác vụ huấn luyện.\nTuy nhiên, trong workshop này chúng ta chọn EC2 vì lý do chi phí — EC2 có thể dùng trong free tier, rẻ và không phát sinh chi phí nếu quản lý thời gian chạy hợp lý. Bài lab này sẽ chỉ hướng dẫn chạy demo cách training một mô hình Transformers để tối ưu các tác vụ NLP.\n"
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/2.-prepare/2.4-set-iam-policy/",
	"title": "Cấu hình quyền IAM",
	"tags": [],
	"description": "",
	"content": "Mục tiêu Tạo và cấu hình IAM Role để cấp quyền truy cập cần thiết cho các dịch vụ AWS (như Lambda, SageMaker, Comprehend) sử dụng dữ liệu trong S3 và thực hiện các tác vụ NLP.\nCác bước thực hiện 1. Tạo IAM Role Truy cập IAM trong AWS Management Console.\nChọn Roles → Create role.\nTrong mục Trusted entity type, chọn:\nAWS service Chọn dịch vụ sẽ dùng role này: Lambda (nếu role cho Lambda) SageMaker (nếu role cho SageMaker)\nNhấn Next để sang bước gán quyền.\n2. Gán quyền (Attach policies) Thêm các policy sau:\nAmazonS3FullAccess – Cho phép đọc/ghi vào S3 bucket ComprehendFullAccess – Cho phép gọi AWS Comprehend API AmazonSageMakerFullAccess – (nếu role cho SageMaker) CloudWatchLogsFullAccess – Ghi log khi chạy Lambda/SageMaker\nNhấn Next để đặt tên role.\n3. Hoàn tất tạo role Nhập tên role, mỗi role làm tương tự các bước trên: NLP-Lambda-Role NLP-SageMaker-Role Nhấn Create role.\nKết quả mong đợi Một IAM Role mới xuất hiện trong danh sách IAM Roles. Role có đầy đủ quyền để truy cập S3, Comprehend, SageMaker và CloudWatch. "
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/5.-clear-resources/",
	"title": "5. Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "5.1 Xóa EC2 Instance Đăng nhập vào AWS Management Console → chọn EC2 → Instances.\nChọn instance bạn đã tạo (ví dụ aws-transformers-model).\nNhấn nút Actions → Instance State → Terminate Instance.\nXác nhận để xóa instance.\nLưu ý: Sau khi terminate, EC2 instance sẽ bị xóa hoàn toàn và không thể phục hồi.\n5.2 Xóa S3 Bucket Vào AWS Management Console → chọn S3.\nChọn bucket bạn đã sử dụng (ví dụ aws-nlp-data).\nTrước khi xóa bucket, cần xóa toàn bộ dữ liệu bên trong: Mở bucket → chọn empty → xác nhận.\nSau khi bucket trống, quay lại trang bucket → chọn bucket → nhấn Delete bucket.\nNhập tên bucket để xác nhận và hoàn tất xóa.\n5.3 Xóa các tài nguyên phụ trợ (IAM Role, Key Pair, \u0026hellip;) Xóa IAM Role Vào AWS Management Console → chọn IAM → Roles.\nTìm role bạn đã tạo cho Lambda hoặc EC2 (ví dụ NLP-Lambda-Role).\nNếu role đang được gán cho resource, hãy tháo gỡ trước (ví dụ Lambda, EC2 profile). Chọn role → nhấn Delete role → xác nhận.\nXóa Key Pair EC2 Vào EC2 Console → bên thanh menu chọn Key Pairs.\nTìm key pair bạn đã tạo (ví dụ transformers-demo-key).\nChọn key pair → nhấn Delete → xác nhận.\nKết luận Sau khi hoàn thành các bước trên, bạn đã dọn dẹp toàn bộ tài nguyên đã tạo trong workshop, tránh phát sinh chi phí không mong muốn và giữ môi trường AWS gọn gàng.\n"
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://necrose01.github.io/Workshop/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]