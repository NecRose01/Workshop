[
{
	"uri": "https://necrose01.github.io/Workshop/1.-introduction/",
	"title": "1. Introduction",
	"tags": [],
	"description": "",
	"content": "What is NLP? Natural Language Processing (NLP) is a field of artificial intelligence focused on enabling computers to understand, interpret, and generate human language.\nNLP is widely applied in many products and services, such as:\nChatbots and virtual assistants Machine Translation Intelligent search Sentiment Analysis Automatic text summarization Topic detection and content classification AWS Comprehend AWS Comprehend is a fully managed natural language processing service from AWS that allows you to extract insights and information from text.\nKey capabilities include:\nSentiment Analysis Entity Recognition Language Detection Text Classification (Custom Classification) Key phrase and keyword extraction With AWS Comprehend, you can perform NLP tasks without training models from scratch, while still supporting customization to fit your data.\nTransformers Transformers are a deep learning architecture based on the Self-Attention mechanism, first introduced in the paper “Attention Is All You Need” (2017).\nTransformers are the foundation of many advanced language models such as BERT, GPT, T5, and RoBERTa.\nSome advantages:\nBetter parallel processing than RNN/LSTM Effective with long sequences Ability to learn contextual and relational information within sentences Popular types of Transformers:\nEncoder-only (BERT, RoBERTa) Decoder-only (GPT, LLaMA) Encoder-Decoder (T5, BART) AWS Transformers on SageMaker AWS provides capabilities to train and deploy Transformer models through Amazon SageMaker.\nYou can:\nUse the Hugging Face Transformers library integrated in SageMaker Customize and fine-tune models on your own data Deploy endpoints for real-time or batch inference Combining SageMaker and AWS Comprehend enables building flexible NLP pipelines that leverage both custom model power and existing AWS NLP APIs.\n"
},
{
	"uri": "https://necrose01.github.io/Workshop/3.-aws-comprehend/3.1-demo-aws-comprehend/",
	"title": "Basic Demo of AWS Comprehend",
	"tags": [],
	"description": "",
	"content": "Objective Perform a quick demo of AWS Comprehend via AWS Console to analyze sentiment and extract information from a sample text.\nSteps to perform 1. Open AWS Comprehend Log in to AWS Management Console. Search for and select the Amazon Comprehend service.\nLaunch the Amazon Comprehend service.\nHere you can test the provided sample text to see how AWS Comprehend analyzes the text; click Analyze to run the analysis.\nAfter analysis, you can view detailed information such as: Entities: Entity recognition Key phrases: Key phrase extraction Language: Language detection PII: Personally Identifiable Information detection Sentiment: Sentiment analysis Targeted sentiment: Sentiment for specific targets Syntax: Syntax analysis\n"
},
{
	"uri": "https://necrose01.github.io/Workshop/2.-prepare/2.1-create-s3-bucket/",
	"title": "Create Amazon S3 Bucket",
	"tags": [],
	"description": "",
	"content": "Objective Create an Amazon S3 bucket to store input data (CSV files) and analysis results for the workshop.\nSteps Sign in to the AWS Management Console.\nNavigate to the Amazon S3 service using the search bar.\nClick Create bucket.\nEnter a bucket name (e.g., aws-nlp-data).\nKeep default settings or customize as needed.\nClick Create bucket to finish.\nExpected result A new bucket appears in the Amazon S3 buckets list. The bucket is ready for uploading CSV data.\n"
},
{
	"uri": "https://necrose01.github.io/Workshop/",
	"title": "Natural Language Processing with Comprehend and Transformers",
	"tags": [],
	"description": "",
	"content": "Natural Language Processing with Comprehend and Transformers Overview This lab guides you to learn and practice Natural Language Processing (NLP) by combining AWS Comprehend and Transformer models.\nThrough the workshop content, you will apply these techniques to analyze customer reviews on an e-commerce platform, thereby extracting useful insights to improve products and services.\nMain Content Introduction Preparation AWS Comprehend Transformers Resource Cleanup "
},
{
	"uri": "https://necrose01.github.io/Workshop/4.-transformers/4.1-create-ec2/",
	"title": "Setting up EC2",
	"tags": [],
	"description": "",
	"content": "1. Preparing the EC2 Instance Go to AWS Management Console → select EC2 → Launch Instance.\nName the Instance: aws-transformers-model.\nChoose AMI: Ubuntu Server 22.04 LTS (Free tier eligible).\nChoose Instance type: t2.micro (Free tier).\nTo train Transformer models with large datasets or high computational requirements, you should select instances with stronger GPU or CPU such as:\ng4dn.xlarge, g5.xlarge (GPU instances)\np3.2xlarge, p4d.24xlarge (Deep learning optimized instances)\nc5.4xlarge, c6i.4xlarge (High performance CPU optimized)\nHere, we use t2.micro to run a demo training with small data to save costs and fit the free tier.\nKey pair: Select or create a new key pair (.pem file), remember to download and keep it to SSH into the instance.\nNetwork: Configure Security Group to allow:\nSelect all 3 options:\nAllow SSH traffic from Allow HTTPS traffic from the internet Allow HTTP traffic from the internet\nStorage: Keep the default 8 GB.\nClick Launch instance to start.\n"
},
{
	"uri": "https://necrose01.github.io/Workshop/2.-prepare/",
	"title": "2. Preparation",
	"tags": [],
	"description": "",
	"content": "Preparation Before starting the workshop, please ensure you have the following resources and data ready:\nAmazon S3 bucket to store data and analysis results. CSV file containing customer review data (including fields: customer name, star rating, product, product category, review date, review content, number of likes). IAM policy configuration Create and assign necessary permissions so that AWS Comprehend and Transformer models can access S3, read data, and write processing results. "
},
{
	"uri": "https://necrose01.github.io/Workshop/3.-aws-comprehend/3.2-create-lambda-function/",
	"title": "Create Lambda Function to Process CSV Files",
	"tags": [],
	"description": "",
	"content": "Objectives Automatically trigger when a new CSV file is uploaded to S3. Read each row in the CSV, call Comprehend DetectSentiment to analyze the sentiment of the review. Generate a result CSV file and save it in the processed/ folder in the same or another bucket. Step 1: Create Lambda Function Go to AWS Lambda Console → Create function.\nChoose Author from scratch:\nName: ComprehendCSVProcessor Runtime: Python 3.13 (or the latest version) Role: Select NLP-Lambda-Role created in section 2.4. Click Create function.\nStep 2: Lambda Code Paste the following code into the Lambda Code source: import boto3 import csv import io import json import os import urllib.parse from datetime import datetime # CONFIG - change according to your environment REGION = \u0026#34;ap-southeast-1\u0026#34; LANGUAGE_CODE = \u0026#34;en\u0026#34; SENSITIVE_WORDS = { # sample list; can be loaded from S3 if you need a larger list \u0026#34;damn\u0026#34;, \u0026#34;shit\u0026#34;, \u0026#34;trash\u0026#34;, \u0026#34;bastard\u0026#34;, \u0026#34;f**k\u0026#34;, \u0026#34;m*\u0026#34; } # Initialize clients s3 = boto3.client(\u0026#34;s3\u0026#34;, region_name=REGION) comprehend = boto3.client(\u0026#34;comprehend\u0026#34;, region_name=REGION) # Map sentiment string -\u0026gt; numeric label (used for training) SENTIMENT_TO_LABEL = { \u0026#34;POSITIVE\u0026#34;: 0, \u0026#34;NEGATIVE\u0026#34;: 1, \u0026#34;NEUTRAL\u0026#34;: 2, \u0026#34;MIXED\u0026#34;: 3, \u0026#34;N/A\u0026#34;: 4 } def normalize_token(t: str): return t.strip().lower() def analyze_text(text: str): \u0026#34;\u0026#34;\u0026#34; Call Comprehend APIs and return a detailed result dictionary. \u0026#34;\u0026#34;\u0026#34; if not text or text.strip() == \u0026#34;\u0026#34;: return { \u0026#34;sentiment\u0026#34;: \u0026#34;N/A\u0026#34;, \u0026#34;sentiment_scores\u0026#34;: {\u0026#34;Positive\u0026#34;: 0, \u0026#34;Negative\u0026#34;: 0, \u0026#34;Neutral\u0026#34;: 0, \u0026#34;Mixed\u0026#34;: 0}, \u0026#34;key_phrases\u0026#34;: [], \u0026#34;entities\u0026#34;: [], \u0026#34;syntax_tokens\u0026#34;: [] } sent = comprehend.detect_sentiment(Text=text, LanguageCode=LANGUAGE_CODE) kp = comprehend.detect_key_phrases(Text=text, LanguageCode=LANGUAGE_CODE) ents = comprehend.detect_entities(Text=text, LanguageCode=LANGUAGE_CODE) syntax = comprehend.detect_syntax(Text=text, LanguageCode=LANGUAGE_CODE) return { \u0026#34;sentiment\u0026#34;: sent.get(\u0026#34;Sentiment\u0026#34;), \u0026#34;sentiment_scores\u0026#34;: sent.get(\u0026#34;SentimentScore\u0026#34;, {}), \u0026#34;key_phrases\u0026#34;: kp.get(\u0026#34;KeyPhrases\u0026#34;, []), \u0026#34;entities\u0026#34;: ents.get(\u0026#34;Entities\u0026#34;, []), \u0026#34;syntax_tokens\u0026#34;: syntax.get(\u0026#34;SyntaxTokens\u0026#34;, []) } def mark_sensitive_tokens(syntax_tokens): \u0026#34;\u0026#34;\u0026#34; From Comprehend syntax tokens, mark tokens found in SENSITIVE_WORDS. Returns a list of token dicts: {\u0026#34;text\u0026#34;:..., \u0026#34;beginOffset\u0026#34;:..., \u0026#34;part_of_speech\u0026#34;:..., \u0026#34;is_sensitive\u0026#34;: True/False} \u0026#34;\u0026#34;\u0026#34; out = [] for tk in syntax_tokens: txt = tk.get(\u0026#34;Text\u0026#34;, \u0026#34;\u0026#34;) pos_tag = tk.get(\u0026#34;PartOfSpeech\u0026#34;, {}).get(\u0026#34;Tag\u0026#34;) is_sens = normalize_token(txt) in SENSITIVE_WORDS out.append({ \u0026#34;text\u0026#34;: txt, \u0026#34;beginOffset\u0026#34;: tk.get(\u0026#34;BeginOffset\u0026#34;), \u0026#34;part_of_speech\u0026#34;: pos_tag, \u0026#34;is_sensitive\u0026#34;: is_sens }) return out def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34; Lambda entry point expects an S3 Put event. For manual testing, craft an event with \u0026#39;bucket\u0026#39; and \u0026#39;key\u0026#39;. \u0026#34;\u0026#34;\u0026#34; try: record = event[\u0026#34;Records\u0026#34;][0] bucket = record[\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = urllib.parse.unquote_plus(record[\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;]) except Exception as e: return {\u0026#34;statusCode\u0026#34;: 400, \u0026#34;body\u0026#34;: f\u0026#34;Invalid event format: {e}\u0026#34;} base_name = os.path.basename(key) timestamp = datetime.utcnow().strftime(\u0026#34;%Y%m%dT%H%M%SZ\u0026#34;) # Temporary local paths download_path = f\u0026#34;/tmp/{base_name}\u0026#34; output_jsonl_path = f\u0026#34;/tmp/processed_{base_name.rsplit(\u0026#39;.\u0026#39;,1)[0]}_{timestamp}.jsonl\u0026#34; output_csv_path = f\u0026#34;/tmp/summary_{base_name.rsplit(\u0026#39;.\u0026#39;,1)[0]}_{timestamp}.csv\u0026#34; # Download CSV from S3 try: s3.download_file(bucket, key, download_path) except Exception as e: return {\u0026#34;statusCode\u0026#34;: 500, \u0026#34;body\u0026#34;: f\u0026#34;Error downloading s3://{bucket}/{key} - {e}\u0026#34;} # Open CSV and process each row jsonl_file = open(output_jsonl_path, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) csv_buffer = io.StringIO() csv_writer = csv.writer(csv_buffer, quoting=csv.QUOTE_MINIMAL) csv_writer.writerow([\u0026#34;id\u0026#34;, \u0026#34;customer\u0026#34;, \u0026#34;review\u0026#34;, \u0026#34;label\u0026#34;, \u0026#34;label_str\u0026#34;, \u0026#34;top_key_phrases\u0026#34;, \u0026#34;sensitive_words\u0026#34;]) processed_count = 0 with open(download_path, newline=\u0026#34;\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as csvfile: reader = csv.DictReader(csvfile) for idx, row in enumerate(reader): processed_count += 1 doc_id = f\u0026#34;{base_name}_{idx}\u0026#34; customer = row.get(\u0026#34;customer\u0026#34;, \u0026#34;\u0026#34;) review_text = row.get(\u0026#34;review\u0026#34;, \u0026#34;\u0026#34;) # Call Comprehend result = analyze_text(review_text) # Token-level sensitive marking tokens = mark_sensitive_tokens(result.get(\u0026#34;syntax_tokens\u0026#34;, [])) # Collect sensitive words found sensitive_found = sorted(list({t[\u0026#34;text\u0026#34;] for t in tokens if t[\u0026#34;is_sensitive\u0026#34;]})) # Collect top key phrases key_phrases_list = [kp.get(\u0026#34;Text\u0026#34;) for kp in result.get(\u0026#34;key_phrases\u0026#34;, [])] entities_list = [{\u0026#34;text\u0026#34;: e.get(\u0026#34;Text\u0026#34;), \u0026#34;type\u0026#34;: e.get(\u0026#34;Type\u0026#34;)} for e in result.get(\u0026#34;entities\u0026#34;, [])] # Document-level label label_str = result.get(\u0026#34;sentiment\u0026#34;, \u0026#34;N/A\u0026#34;) label_num = SENTIMENT_TO_LABEL.get(label_str, 4) # JSONL entry doc = { \u0026#34;id\u0026#34;: doc_id, \u0026#34;customer\u0026#34;: customer, \u0026#34;text\u0026#34;: review_text, \u0026#34;label\u0026#34;: label_num, \u0026#34;label_str\u0026#34;: label_str, \u0026#34;sentiment_scores\u0026#34;: result.get(\u0026#34;sentiment_scores\u0026#34;, {}), \u0026#34;key_phrases\u0026#34;: key_phrases_list, \u0026#34;entities\u0026#34;: entities_list, \u0026#34;tokens\u0026#34;: tokens } jsonl_file.write(json.dumps(doc, ensure_ascii=False) + \u0026#34;\\n\u0026#34;) # CSV summary row csv_writer.writerow([ doc_id, customer, review_text, label_num, label_str, \u0026#34;|\u0026#34;.join(key_phrases_list), \u0026#34;|\u0026#34;.join(sensitive_found) ]) jsonl_file.close() # Save CSV file locally as well with open(output_csv_path, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;, newline=\u0026#34;\u0026#34;) as f: f.write(csv_buffer.getvalue()) # Upload outputs back to S3 output_jsonl_key = f\u0026#34;processed/{os.path.basename(output_jsonl_path)}\u0026#34; output_csv_key = f\u0026#34;processed/{os.path.basename(output_csv_path)}\u0026#34; try: s3.upload_file(output_jsonl_path, bucket, output_jsonl_key) s3.upload_file(output_csv_path, bucket, output_csv_key) except Exception as e: return {\u0026#34;statusCode\u0026#34;: 500, \u0026#34;body\u0026#34;: f\u0026#34;Error uploading results to S3: {e}\u0026#34;} return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: { \u0026#34;message\u0026#34;: \u0026#34;Processing complete\u0026#34;, \u0026#34;processed_count\u0026#34;: processed_count, \u0026#34;jsonl_s3\u0026#34;: f\u0026#34;s3://{bucket}/{output_jsonl_key}\u0026#34;, \u0026#34;csv_s3\u0026#34;: f\u0026#34;s3://{bucket}/{output_csv_key}\u0026#34; } } Select Deploy (or press Ctrl + Shift + U) to save and deploy the code.\nStep 3: Create Test Event Open AWS Lambda Console → select the function ComprehendCSVProcessor.\nClick Test → choose Create new event.\nChoose Event template: search for \u0026quot;Amazon S3 Put\u0026quot; (or \u0026quot;S3 Put\u0026quot;).\nModify the JSON as below (replace with your bucket name and file key): { \u0026#34;Records\u0026#34;: [ { \u0026#34;s3\u0026#34;: { \u0026#34;bucket\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;bucket-name\u0026#34; }, \u0026#34;object\u0026#34;: { \u0026#34;key\u0026#34;: \u0026#34;reviews.csv\u0026#34; } } } ] } The \u0026ldquo;key\u0026rdquo; must exactly match the path and filename in S3 (e.g., \u0026ldquo;input/customer_reviews.csv\u0026rdquo;).\nIf the filename contains spaces or special characters, it will be encoded (space → %20). Please copy it exactly from the S3 console.\nStep 4: Run Test and Check Results Click Test → Lambda will read the file from S3 and process it. After completion: Open S3 Console. Navigate to the processed/ folder to view the resulting CSV files.\nHere, two files are output: a JSONL file for training and a CSV file for flexible use.\n"
},
{
	"uri": "https://necrose01.github.io/Workshop/2.-prepare/2.2-create-csv-file/",
	"title": "Prepare and Upload CSV File to S3",
	"tags": [],
	"description": "",
	"content": "Objective Prepare a CSV file containing customer review data from a database or existing file, ensuring the correct format so that AWS Comprehend and Transformer models can process it. Then upload this CSV file to the S3 bucket created in step 2.1.\nPart 1 – CSV File Configuration Data column structure\nThe CSV file should have the following columns:\ncustomer_name – Customer name rating – Star rating (1–5) product_name – Product name product_category – Product category review_date – Review date (YYYY-MM-DD) review_text – Review content likes – Number of likes on the comment To optimize the model, you may simplify the CSV to only contain the necessary textual data for the model, avoiding unnecessary text. Here, the CSV file keeps only two columns: customer and review.\nSample CSV \u0026#34;customer\u0026#34;,\u0026#34;review\u0026#34; \u0026#34;Nguyen Van A\u0026#34;,\u0026#34;The joystick is very responsive, gaming feels amazing, worth the money!\u0026#34; \u0026#34;Le Thi B\u0026#34;,\u0026#34;Beautiful, sturdy, works well with both PC and phone.\u0026#34; \u0026#34;Tran Van C\u0026#34;,\u0026#34;Broke after just a few days of use, extremely disappointed!\u0026#34; \u0026#34;Pham Thi D\u0026#34;,\u0026#34;Weak battery, unstable Bluetooth connection.\u0026#34; \u0026#34;Vo Quoc E\u0026#34;,\u0026#34;.\u0026#34; \u0026#34;Ngo My F\u0026#34;,\u0026#34;??\u0026#34; \u0026#34;Do Nhat G\u0026#34;,\u0026#34;I just had pho, it was delicious!\u0026#34; \u0026#34;Hoang T H\u0026#34;,\u0026#34;The weather is so nice today that I decided to buy it.\u0026#34; \u0026#34;Trinh Manh I\u0026#34;,\u0026#34;D*mn this joystick makes me so mad!\u0026#34; \u0026#34;Bui Gia K\u0026#34;,\u0026#34;Trash! Selling like this will get you in serious trouble!!!\u0026#34; Format \u0026amp; Encoding\nFormat: .csv (comma-separated values) Encoding: UTF-8 to avoid Vietnamese character issues First line: header row containing column names Part 2 – Convert Database Table to CSV If your data is in a database (MySQL, PostgreSQL, etc.), you can export it to CSV:\nMySQL SELECT customer_name, review_text FROM customer_reviews INTO OUTFILE \u0026#39;/tmp/customer_reviews.csv\u0026#39; FIELDS TERMINATED BY \u0026#39;,\u0026#39; ENCLOSED BY \u0026#39;\u0026#34;\u0026#39; LINES TERMINATED BY \u0026#39;\\n\u0026#39;; "
},
{
	"uri": "https://necrose01.github.io/Workshop/4.-transformers/4.2-setup-environment/",
	"title": "Setup Environment on EC2",
	"tags": [],
	"description": "",
	"content": "Connect to EC2 Instance and Set Up Environment Go to AWS Management Console → select EC2 → Select the created Instance → Connect.\nUpdate packages and install pip, git, curl. After running, wait a moment for the update and installation to complete.\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y sudo apt install -y python3-pip git curl Install Transformers \u0026amp; dependencies Set up environment python3 -m venv venv source venv/bin/activate Install Transformers pip install --upgrade pip pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu pip install transformers datasets scikit-learn pandas Download CSV File from S3 Install AWS CLI # download AWS CLI curl \u0026#34;https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\u0026#34; -o \u0026#34;awscliv2.zip\u0026#34; # install unzip tool sudo apt install unzip # unzip the downloaded file unzip awscliv2.zip # install AWS CLI sudo ./aws/install # verify AWS CLI installation aws --version Download CSV file to Instance:\nMake sure the user has at least AmazonS3ReadOnlyAccess permissions.\nEnsure the Instance is assigned an IAM role that allows S3 access; without this, it cannot access S3 to download files.\n# configure aws: only input the region you are using here (e.g., ap-southeast-1), leave others blank. aws configure # download the CSV file aws s3 cp s3://aws-nlp-data/processed/summary_reviews_20250809T124112Z.csv "
},
{
	"uri": "https://necrose01.github.io/Workshop/4.-transformers/4.3-training-model/",
	"title": "Training model",
	"tags": [],
	"description": "",
	"content": "1. Create train.py file Open a text editor on EC2 (e.g., nano or vim): nano train.py Nano text editor interface:\nCopy the following source code, then save it\nimport pandas as pd from sklearn.model_selection import train_test_split from datasets import Dataset from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer import torch # 1. Load dataset CSV df = pd.read_csv(\u0026#34;summary_reviews_20250809T124112Z.csv\u0026#34;) # change CSV filename downloaded from S3 df = df.rename(columns={\u0026#34;label\u0026#34;: \u0026#34;labels\u0026#34;}) # Transformers requires \u0026#34;labels\u0026#34; column name # 2. Train/Test split train_df, test_df = train_test_split(df, test_size=0.2, random_state=42) train_dataset = Dataset.from_pandas(train_df) test_dataset = Dataset.from_pandas(test_df) # 3. Load tokenizer \u0026amp; small model for t2.micro model_name = \u0026#34;distilbert-base-uncased\u0026#34; tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3) # 4. Tokenize def tokenize(batch): return tokenizer(batch[\u0026#34;review\u0026#34;], padding=True, truncation=True, max_length=64) train_dataset = train_dataset.map(tokenize, batched=True) test_dataset = test_dataset.map(tokenize, batched=True) # 5. Training arguments training_args = TrainingArguments( output_dir=\u0026#34;./results\u0026#34;, # evaluation_strategy=\u0026#34;epoch\u0026#34;, num_train_epochs=3, per_device_train_batch_size=1, per_device_eval_batch_size=1, logging_dir=\u0026#34;./logs\u0026#34;, logging_steps=1 ) # 6. Metrics from sklearn.metrics import accuracy_score, precision_recall_fscore_support def compute_metrics(p): preds = p.predictions.argmax(-1) precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average=\u0026#34;weighted\u0026#34;) acc = accuracy_score(p.label_ids, preds) return {\u0026#34;accuracy\u0026#34;: acc, \u0026#34;precision\u0026#34;: precision, \u0026#34;recall\u0026#34;: recall, \u0026#34;f1\u0026#34;: f1} # 7. Trainer trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=test_dataset, tokenizer=tokenizer, compute_metrics=compute_metrics ) # 8. Train \u0026amp; Evaluate trainer.train() metrics = trainer.evaluate() print(metrics) # 9. Save model trainer.save_model(\u0026#34;./final_model\u0026#34;) print(\u0026#34;Model saved to ./final_model\u0026#34;) 2. Run train.py Reinstall transformers and dependencies pip install --upgrade \u0026#34;transformers[torch]\u0026#34; accelerate Run the following command in the folder containing train.py and CSV file. Training will take some time.\nBecause the instance is a very weak t2.micro, training may be killed unexpectedly. If this happens, upgrade instance type gradually to t2.medium, then t2.large to reduce cost. If all low-cost types fail, upgrading to a more expensive type is necessary. Training NLP models requires strong CPU/GPU, so some cost is inevitable. Alternatively, run training on a local machine if powerful enough.\nThis lab only demonstrates training a model, with a very small dataset, so overfitting is expected. For effective training, a large dataset is required instead of small incremental batches.\npython3 train.py 3. Training Model Results The text classification training process with distilbert-base-uncased was successfully completed on the EC2 instance with a small dataset.\nThe model was trained for 3 epochs, with total training time around 7.7 seconds on a t2.micro instance.\nLoss fluctuated during training, showing the model learned information from data (final train_loss approx 0.94).\nAfter training, the model was successfully saved in the ./final_model directory.\nModel evaluation results on test set: Metric Value Loss 0.875 Accuracy 0.5 Precision 0.25 Recall 0.5 F1-score 0.33 Some classifier layer weights were newly initialized because the base model didn\u0026rsquo;t have this part, so further training on this specific task is needed. Metric warnings due to small data and imbalanced label distribution are normal in limited data scenarios. Accuracy and metrics are low mainly due to small dataset size, but the model shows basic learning capability. Training and model saving were successful; the model is ready for further fine-tuning or inference. 4. Upload trained model to S3 Upload the trained final_model to S3 aws s3 cp ./final_model s3://aws-nlp-data/transformers-demo/ --recursive The final_model is now on S3, ready to be downloaded, inspected, used, or further trained.\n"
},
{
	"uri": "https://necrose01.github.io/Workshop/3.-aws-comprehend/",
	"title": "3. AWS Comprehend",
	"tags": [],
	"description": "",
	"content": "Overview Amazon Comprehend is a fully managed Natural Language Processing (NLP) service by AWS that helps analyze and extract information from text without the need to build or train complex AI models.\nComprehend uses machine learning technology to automatically identify:\nThe language of the text. Sentiment – positive, negative, neutral, mixed. Entities – people names, locations, organizations, dates, etc. Key phrases. Topics and text classification. "
},
{
	"uri": "https://necrose01.github.io/Workshop/2.-prepare/2.3-upload-csv-to-s3-bucket/",
	"title": "Upload CSV to S3",
	"tags": [],
	"description": "",
	"content": "Objective Upload the CSV file containing customer review data to the Amazon S3 bucket created earlier, and verify the data has been successfully stored.\nSteps Open Amazon S3 in the AWS Management Console.\nSelect the bucket created in step 2.1 – Create Amazon S3 Bucket.\nClick Upload.\nUnder Add files, select the CSV file prepared in step 2.2 – Prepare and Configure CSV.\nKeep default settings (or customize if needed), then click Upload to start uploading.\nVerify Upload After a successful upload, you will see the CSV file listed under the bucket\u0026rsquo;s Objects.\nExpected Result The CSV file appears in the bucket\u0026rsquo;s object list in S3. You can view or inspect the file content directly from S3. "
},
{
	"uri": "https://necrose01.github.io/Workshop/4.-transformers/",
	"title": "4 . Transformers",
	"tags": [],
	"description": "",
	"content": "Overview Transformers is a deep learning architecture specialized for natural language processing (NLP) and currently serves as the foundation for many advanced AI models such as BERT, GPT, T5, RoBERTa.\nA key advantage of Transformers is their ability to handle parallel processing and capture long-range context, which speeds up training and improves prediction quality.\nRecap on Amazon Comprehend Amazon Comprehend is powerful enough to perform general text analysis tasks (since it is essentially an NLP service).\nHowever, to customize and optimize for a specific purpose such as e-commerce data analysis, we will need to use Transformers to train and fine-tune a model, enabling deeper and more domain-specific analysis.\nWe will:\nUse Amazon EC2 to train a Transformer model for commerce analysis. Store and manage the model on S3 for reuse or deployment. You can use SageMaker Notebook to train models. SageMaker is more user-friendly, integrates many optimization tools, and is more efficient than EC2 for training tasks.\nHowever, in this workshop, we choose EC2 due to cost reasons — EC2 can be used within the free tier, is cheaper, and incurs no cost if runtime is managed properly.\nThis lab will only demonstrate how to run a training demo of a Transformer model to optimize NLP tasks.\n"
},
{
	"uri": "https://necrose01.github.io/Workshop/2.-prepare/2.4-set-iam-policy/",
	"title": "IAM Role Configuration",
	"tags": [],
	"description": "",
	"content": "Objective Create and configure an IAM Role to grant necessary permissions for AWS services (such as Lambda, SageMaker, Comprehend) to access data in S3 and perform NLP tasks.\nSteps to Follow 1. Create IAM Role Go to IAM in the AWS Management Console.\nSelect Roles → Create role.\nUnder Trusted entity type, choose:\nAWS service Select the service that will use this role: Lambda (if role is for Lambda) SageMaker (if role is for SageMaker)\nClick Next to proceed to attaching policies.\n2. Attach Permissions (Attach policies) Add the following policies:\nAmazonS3FullAccess – Allows read/write access to S3 buckets ComprehendFullAccess – Allows calling AWS Comprehend API AmazonSageMakerFullAccess – (if role is for SageMaker) CloudWatchLogsFullAccess – Allows logging for Lambda/SageMaker runs\nClick Next to name the role.\n3. Complete Role Creation Enter the role name, repeat for each role needed: NLP-Lambda-Role NLP-SageMaker-Role Click Create role.\nExpected Outcome A new IAM Role appears in the IAM Roles list. The role has full permissions to access S3, Comprehend, SageMaker, and CloudWatch.\n"
},
{
	"uri": "https://necrose01.github.io/Workshop/5.-clear-resources/",
	"title": "5. Clean up Resources",
	"tags": [],
	"description": "",
	"content": "5.1 Delete EC2 Instance Log in to AWS Management Console → select EC2 → Instances.\nSelect the instance you created (e.g., aws-transformers-model).\nClick Actions → Instance State → Terminate Instance.\nConfirm to delete the instance.\nNote: After termination, the EC2 instance will be permanently deleted and cannot be recovered.\n5.2 Delete S3 Bucket Go to AWS Management Console → select S3.\nSelect the bucket you used (e.g., aws-nlp-data).\nBefore deleting the bucket, you need to delete all data inside: Open the bucket → choose empty → confirm.\nAfter the bucket is empty, go back to the bucket page → select the bucket → click Delete bucket.\nEnter the bucket name to confirm and complete deletion.\n5.3 Delete Supporting Resources (IAM Role, Key Pair, \u0026hellip;) Delete IAM Role Go to AWS Management Console → select IAM → Roles.\nFind the role you created for Lambda or EC2 (e.g., NLP-Lambda-Role).\nIf the role is attached to any resource, detach it first (e.g., Lambda, EC2 profile). Select the role → click Delete role → confirm.\nDelete EC2 Key Pair Go to EC2 Console → in the sidebar menu choose Key Pairs.\nFind the key pair you created (e.g., transformers-demo-key).\nSelect the key pair → click Delete → confirm.\nConclusion After completing the above steps, you have cleaned up all the resources created during the workshop, avoiding unwanted costs and keeping your AWS environment tidy.\n"
},
{
	"uri": "https://necrose01.github.io/Workshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://necrose01.github.io/Workshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]